use crate::{
    domain::{
        article::{search_unprocessed_rss_links, store_article, Article},
        feed::{search_feeds, Feed, FeedQuery},
        rss::{extract_rss_links_from_channel, store_rss_links, RssLink},
    },
    infra::{api::http::HttpClientProtocol, parser::parse_channel_from_xml_str},
};
use anyhow::{Context, Result};
use sqlx::PgPool;

// --- Production/Online Test Imports ---
#[cfg(any(not(test), feature = "online"))]
use crate::{
    domain::article::fetch_article_from_url, infra::api::http::ReqwestHttpClient as HttpClient,
};

// --- Offline Test Imports ---
#[cfg(all(test, not(feature = "online")))]
use crate::{
    domain::article::fetch_article_with_client,
    infra::api::{firecrawl::MockFirecrawlClient, http::MockHttpClient as HttpClient},
};

/// RSSãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
///
/// 1. feeds.yamlã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
/// 2. å„RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜
/// 3. æœªå‡¦ç†ã®ãƒªãƒ³ã‚¯ã‹ã‚‰è¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜
///
/// # å¼•æ•°
/// * `pool` - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
/// * `group` - å‡¦ç†å¯¾è±¡ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆNoneã®å ´åˆã¯å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å‡¦ç†ï¼‰
pub async fn execute_rss_workflow(pool: &PgPool, group: Option<&str>) -> Result<()> {
    match group {
        Some(group_name) => {
            println!("=== RSSãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—: {}ï¼‰===", group_name);
        }
        None => {
            println!("=== RSSãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ ===");
        }
    }

    // feeds.yamlã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    let query = group.map(|g| FeedQuery {
        group: Some(g.to_string()),
        name: None,
    });
    let feeds = search_feeds(query).context("ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")?;

    if let Some(group_name) = group {
        if feeds.is_empty() {
            println!(
                "æŒ‡å®šã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ— '{}' ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                group_name
            );
            return Ok(());
        }
        println!("å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ‰æ•°: {}ä»¶", feeds.len());
    } else {
        println!("ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {}ä»¶", feeds.len());
    }

    // HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    #[cfg(any(not(test), feature = "online"))]
    let http_client = HttpClient::new();

    #[cfg(all(test, not(feature = "online")))]
    let http_client = HttpClient::new_success("<rss><channel><item><title>ãƒ†ã‚¹ãƒˆ</title><link>https://test.com</link></item></channel></rss>");

    // æ®µéš1: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—
    process_collect_rss_links(&http_client, &feeds, pool).await?;
    // æ®µéš2: æœªå‡¦ç†ã®ãƒªãƒ³ã‚¯ã‹ã‚‰è¨˜äº‹å†…å®¹ã‚’å–å¾—
    process_collect_backlog_articles(pool).await?;

    match group {
        Some(group_name) => {
            println!("=== RSSãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—: {}ï¼‰===", group_name);
        }
        None => {
            println!("=== RSSãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº† ===");
        }
    }
    Ok(())
}

/// RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’åé›†ã—ã¦DBã«ä¿å­˜ã™ã‚‹
async fn process_collect_rss_links(
    client: &dyn HttpClientProtocol,
    feeds: &[Feed],
    pool: &PgPool,
) -> Result<()> {
    println!("--- RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯å–å¾—é–‹å§‹ ---");

    for feed in feeds {
        println!("ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†ä¸­: {} - {}", feed.group, feed.name);

        match fetch_rss_links_from_feed(client, feed).await {
            Ok(rss_links) => {
                println!("  {}ä»¶ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º", rss_links.len());

                match store_rss_links(&rss_links, pool).await {
                    Ok(result) => {
                        println!("  DBä¿å­˜çµæœ: {}", result);
                    }
                    Err(e) => {
                        eprintln!("  DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {}", e);
                    }
                }
            }
            Err(e) => {
                eprintln!("  ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {}", e);
            }
        }
    }

    println!("--- RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯å–å¾—å®Œäº† ---");
    Ok(())
}

/// feedã‹ã‚‰rss_linkã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
async fn fetch_rss_links_from_feed(
    client: &dyn HttpClientProtocol,
    feed: &Feed,
) -> Result<Vec<RssLink>> {
    let xml_content = client
        .get_text(&feed.link, 30)
        .await
        .context(format!("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—: {}", feed.link))?;
    let channel = parse_channel_from_xml_str(&xml_content).context("XMLã®è§£æã«å¤±æ•—")?;
    let rss_links = extract_rss_links_from_channel(&channel);

    Ok(rss_links)
}

/// æœªå‡¦ç†ã®ãƒªãƒ³ã‚¯ã‹ã‚‰å‡¦ç†å¾…ã¡ã®è¨˜äº‹ã‚’åé›†ã—ã¦DBã«ä¿å­˜ã™ã‚‹
async fn process_collect_backlog_articles(pool: &PgPool) -> Result<()> {
    println!("--- è¨˜äº‹å†…å®¹å–å¾—é–‹å§‹ ---");
    // æœªå‡¦ç†ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆarticleãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã—ãªã„rss_linkã‚’å–å¾—ï¼‰
    let unprocessed_links = search_unprocessed_rss_links(pool).await?;
    println!("æœªå‡¦ç†ãƒªãƒ³ã‚¯æ•°: {}ä»¶", unprocessed_links.len());

    for rss_link in unprocessed_links {
        println!("è¨˜äº‹å‡¦ç†ä¸­: {}", rss_link.link);

        let article_result = {
            #[cfg(all(test, not(feature = "online")))]
            {
                // é€šå¸¸ãƒ†ã‚¹ãƒˆæ™‚ã¯ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                let mock_client = MockFirecrawlClient::new_success("ãƒ†ã‚¹ãƒˆè¨˜äº‹å†…å®¹");
                fetch_article_with_client(&rss_link.link, &mock_client).await
            }
            #[cfg(any(not(test), feature = "online"))]
            {
                // æœ¬ç•ªå®Ÿè¡Œæ™‚ã¾ãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆæ™‚ã¯å®Ÿéš›ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                fetch_article_from_url(&rss_link.link).await
            }
        };

        match article_result {
            Ok(article) => match store_article(&article, pool).await {
                Ok(result) => {
                    println!("  è¨˜äº‹ä¿å­˜çµæœ: {}", result);
                }
                Err(e) => {
                    eprintln!("  è¨˜äº‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {}", e);
                }
            },
            Err(e) => {
                eprintln!("  è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {}", e);

                // ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ã€status_codeã‚’è¨˜éŒ²ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                let error_article = Article {
                    url: rss_link.link,
                    timestamp: chrono::Utc::now(),
                    status_code: 500, // ã‚¨ãƒ©ãƒ¼ç”¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰
                    content: format!("å–å¾—ã‚¨ãƒ©ãƒ¼: {}", e),
                };

                if let Err(store_err) = store_article(&error_article, pool).await {
                    eprintln!("  ã‚¨ãƒ©ãƒ¼è¨˜äº‹ã®ä¿å­˜ã«å¤±æ•—: {}", store_err);
                }
            }
        }
    }

    println!("--- è¨˜äº‹å†…å®¹å–å¾—å®Œäº† ---");
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use sqlx::PgPool;

    /// åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‹•ä½œãƒ†ã‚¹ãƒˆ
    mod basic_workflow_tests {
        use super::*;
        use crate::infra::api::http::MockHttpClient;

        #[sqlx::test]
        async fn test_empty_feeds_processing(_pool: PgPool) -> Result<(), anyhow::Error> {
            // ç©ºã®ãƒ•ã‚£ãƒ¼ãƒ‰é…åˆ—ã®ãƒ†ã‚¹ãƒˆ
            let empty_feeds: Vec<Feed> = vec![];
            let mock_client = MockHttpClient::new_success("");
            let result = process_collect_rss_links(&mock_client, &empty_feeds, &_pool).await;

            assert!(result.is_ok(), "ç©ºãƒ•ã‚£ãƒ¼ãƒ‰ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã¯ãš");
            println!("âœ… ç©ºãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }

        #[sqlx::test]
        async fn test_empty_backlog_articles(pool: PgPool) -> Result<(), anyhow::Error> {
            // æœªå‡¦ç†ãƒªãƒ³ã‚¯ãŒ0ä»¶ã®å ´åˆã®ãƒ†ã‚¹ãƒˆ
            let result = process_collect_backlog_articles(&pool).await;

            assert!(result.is_ok(), "æœªå‡¦ç†ãƒªãƒ³ã‚¯ãŒ0ä»¶ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã¯ãš");
            println!("âœ… ç©ºã®æœªå‡¦ç†ãƒªãƒ³ã‚¯å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }
    }

    /// çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ä½¿ç”¨ï¼‰
    mod integration_tests {
        use super::*;

        #[sqlx::test]
        async fn test_article_fetch_with_mock(pool: PgPool) -> Result<(), anyhow::Error> {
            // ãƒ†ã‚¹ãƒˆç”¨RSSãƒªãƒ³ã‚¯ã‚’æŒ¿å…¥
            sqlx::query!(
                "INSERT INTO rss_links (link, title, pub_date) VALUES ($1, $2, CURRENT_TIMESTAMP)",
                "https://test.example.com/article",
                "ãƒ¢ãƒƒã‚¯çµ±åˆãƒ†ã‚¹ãƒˆè¨˜äº‹"
            )
            .execute(&pool)
            .await?;

            // è¨˜äº‹å–å¾—ã‚’å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ä½¿ç”¨ï¼‰
            let result = process_collect_backlog_articles(&pool).await;

            assert!(result.is_ok(), "è¨˜äº‹å–å¾—å‡¦ç†ãŒå¤±æ•—");

            // è¨˜äº‹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
            let article_count = sqlx::query_scalar!("SELECT COUNT(*) FROM articles")
                .fetch_one(&pool)
                .await?;

            assert!(article_count.unwrap_or(0) >= 1, "è¨˜äº‹ãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„");

            println!("âœ… ãƒ¢ãƒƒã‚¯è¨˜äº‹å–å¾—çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }
    }

    /// ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    mod error_handling_tests {
        use super::*;

        #[sqlx::test]
        async fn test_invalid_url_with_mock(pool: PgPool) -> Result<(), anyhow::Error> {
            // ç„¡åŠ¹ãªURLã‚’å«ã‚€RSSãƒªãƒ³ã‚¯ã‚’æŒ¿å…¥
            sqlx::query!(
                "INSERT INTO rss_links (link, title, pub_date) VALUES ($1, $2, CURRENT_TIMESTAMP)",
                "invalid-url",
                "ç„¡åŠ¹URLãƒ†ã‚¹ãƒˆ"
            )
            .execute(&pool)
            .await?;

            let result = process_collect_backlog_articles(&pool).await;

            // ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã¯ç¶™ç¶šã™ã‚‹ã“ã¨
            assert!(
                result.is_ok(),
                "ç„¡åŠ¹URLãŒã‚ã£ã¦ã‚‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã¯æˆåŠŸã™ã‚‹ã¹ã"
            );

            // ãƒ†ã‚¹ãƒˆæ™‚ã¯ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæˆåŠŸã‚’è¿”ã™ã®ã§è¨˜äº‹ãŒä¿å­˜ã•ã‚Œã‚‹
            let article_count = sqlx::query_scalar!("SELECT COUNT(*) FROM articles")
                .fetch_one(&pool)
                .await?;

            assert!(article_count.unwrap_or(0) >= 1, "è¨˜äº‹ãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„");

            println!("âœ… ç„¡åŠ¹URLå‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ˆãƒ¢ãƒƒã‚¯ã§æˆåŠŸï¼‰");
            Ok(())
        }
    }

    /// HTTPãƒ¢ãƒƒã‚¯ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆ
    mod http_mock_tests {
        use super::*;
        use crate::infra::api::http::MockHttpClient;

        #[tokio::test]
        async fn test_fetch_rss_links_with_mock() -> Result<(), anyhow::Error> {
            let rss_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
    <channel>
        <title>ãƒ†ã‚¹ãƒˆRSSãƒ•ã‚£ãƒ¼ãƒ‰</title>
        <item>
            <title>è¨˜äº‹1</title>
            <link>https://example.com/article1</link>
            <pubDate>Wed, 01 Jan 2025 12:00:00 GMT</pubDate>
        </item>
        <item>
            <title>è¨˜äº‹2</title>
            <link>https://example.com/article2</link>
            <pubDate>Thu, 02 Jan 2025 12:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>"#;

            // ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è¨­å®š
            let mock_client = MockHttpClient::new_success(rss_xml);

            let test_feed = Feed {
                group: "test".to_string(),
                name: "ãƒ†ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰".to_string(),
                link: "https://example.com/rss.xml".to_string(),
            };

            let result = fetch_rss_links_from_feed(&mock_client, &test_feed).await;

            assert!(result.is_ok(), "RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ãŒå¤±æ•—");

            let rss_links = result.unwrap();
            assert_eq!(rss_links.len(), 2, "2ä»¶ã®ãƒªãƒ³ã‚¯ãŒå–å¾—ã•ã‚Œã‚‹ã¹ã");

            let first_link = &rss_links[0];
            assert_eq!(first_link.link, "https://example.com/article1");
            assert_eq!(first_link.title, "è¨˜äº‹1");

            println!("âœ… HTTPãƒ¢ãƒƒã‚¯ä½¿ç”¨ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }

        #[tokio::test]
        async fn test_fetch_rss_links_with_error_mock() -> Result<(), anyhow::Error> {
            // ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
            let error_client = MockHttpClient::new_error("æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ");

            let test_feed = Feed {
                group: "test".to_string(),
                name: "ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰".to_string(),
                link: "https://example.com/error.xml".to_string(),
            };

            let result = fetch_rss_links_from_feed(&error_client, &test_feed).await;

            assert!(result.is_err(), "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¹ã");
            let error_msg = result.unwrap_err().to_string();
            println!("ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {}", error_msg);
            // ã‚¨ãƒ©ãƒ¼ãŒæ­£ã—ãä¼æ’­ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert!(error_msg.contains("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—"));

            println!("âœ… HTTPãƒ¢ãƒƒã‚¯ä½¿ç”¨ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }

        #[tokio::test]
        async fn test_fetch_rss_links_with_invalid_xml() -> Result<(), anyhow::Error> {
            let invalid_xml = "<invalid>xml content</broken>";

            let mock_client = MockHttpClient::new_success(invalid_xml);

            let test_feed = Feed {
                group: "test".to_string(),
                name: "ç„¡åŠ¹XMLãƒ†ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰".to_string(),
                link: "https://example.com/invalid.xml".to_string(),
            };

            let result = fetch_rss_links_from_feed(&mock_client, &test_feed).await;

            // XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¹ã
            assert!(result.is_err(), "ç„¡åŠ¹ãªXMLã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¹ã");

            println!("âœ… ç„¡åŠ¹XMLãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }

        #[sqlx::test]
        async fn test_process_collect_rss_links_with_mock(
            pool: PgPool,
        ) -> Result<(), anyhow::Error> {
            let rss_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
    <channel>
        <title>çµ±åˆãƒ†ã‚¹ãƒˆç”¨RSS</title>
        <item>
            <title>çµ±åˆãƒ†ã‚¹ãƒˆè¨˜äº‹</title>
            <link>https://integration.test.com/article</link>
            <pubDate>Fri, 03 Jan 2025 12:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>"#;

            let mock_client = MockHttpClient::new_success(rss_xml);

            let test_feeds = vec![Feed {
                group: "integration".to_string(),
                name: "çµ±åˆãƒ†ã‚¹ãƒˆ".to_string(),
                link: "https://integration.test.com/rss.xml".to_string(),
            }];

            let result = process_collect_rss_links(&mock_client, &test_feeds, &pool).await;

            assert!(result.is_ok(), "RSSåé›†å‡¦ç†ãŒå¤±æ•—");

            // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«RSSãƒªãƒ³ã‚¯ãŒä¿å­˜ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
            let link_count = sqlx::query_scalar!(
                "SELECT COUNT(*) FROM rss_links WHERE link = $1",
                "https://integration.test.com/article"
            )
            .fetch_one(&pool)
            .await?;

            assert!(link_count.unwrap_or(0) >= 1, "RSSãƒªãƒ³ã‚¯ãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„");

            println!("âœ… HTTPãƒ¢ãƒƒã‚¯ä½¿ç”¨ã®çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†");
            Ok(())
        }
    }

    /// é‡ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆonline-slowãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ç”¨ï¼‰
    #[cfg(feature = "online-slow")]
    mod online_slow_tests {
        use super::*;

        /// å®Ÿéš›ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä½¿ã£ãŸå®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ
        #[sqlx::test]
        async fn test_workflow_online_integration(pool: PgPool) -> Result<(), anyhow::Error> {
            // è»½é‡ãªRSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆhttpbin.orgãªã©ï¼‰ã‚’ä½¿ç”¨
            let test_feed = Feed {
                group: "test-online".to_string(),
                name: "httpbin".to_string(),
                link: "https://httpbin.org/xml".to_string(),
            };

            let test_feeds = vec![test_feed];

            // å®Ÿéš›ã®HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦çµ±åˆãƒ†ã‚¹ãƒˆ
            let http_client = ReqwestHttpClient::new();
            let result = process_collect_rss_links(&http_client, &test_feeds, &pool).await;

            match result {
                Ok(()) => {
                    println!("âœ… ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ: RSSãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†å®Œäº†");
                }
                Err(e) => {
                    println!("âš ï¸ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆ: {}", e);
                    println!("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã¾ãŸã¯å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™");
                    // å¤–éƒ¨ä¾å­˜ã®å•é¡Œã¯å¤±æ•—ã«ã—ãªã„
                }
            }

            Ok(())
        }

        /// å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆï¼ˆéå¸¸ã«é‡ã„ï¼‰
        #[sqlx::test]
        async fn test_full_workflow_online(pool: PgPool) -> Result<(), anyhow::Error> {
            println!("ğŸš¨ å®Œå…¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰");

            // ãƒ†ã‚¹ãƒˆç”¨ã®è»½é‡ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®š
            let lightweight_feeds = vec![Feed {
                group: "test-online".to_string(),
                name: "sample".to_string(),
                link: "https://httpbin.org/xml".to_string(),
            }];

            // HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
            let http_client = ReqwestHttpClient::new();

            // æ®µéš1: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒªãƒ³ã‚¯å–å¾—ï¼ˆå®Ÿéš›ã®å¤–éƒ¨é€šä¿¡ï¼‰
            let rss_result =
                process_collect_rss_links(&http_client, &lightweight_feeds, &pool).await;

            match rss_result {
                Ok(()) => {
                    println!("âœ… ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RSSãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†æˆåŠŸ");

                    // æ®µéš2: è¨˜äº‹å†…å®¹å–å¾—ï¼ˆå¤–éƒ¨APIã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã«ã‚ˆã‚Šåˆ¶é™çš„ã«å®Ÿè¡Œï¼‰
                    println!("ğŸ“„ è¨˜äº‹å†…å®¹å–å¾—ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAPIåˆ¶é™è€ƒæ…®ï¼‰");

                    println!("âœ… å®Œå…¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†");
                }
                Err(e) => {
                    println!("âš ï¸ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå•é¡Œ: {}", e);
                    println!("å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™");
                }
            }

            Ok(())
        }
    }
}
